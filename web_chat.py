"""
å•æ–‡ä»¶ç½‘é¡µèŠå¤©: Flask åç«¯ + å†…åµŒå‰ç«¯
å¯åŠ¨: python web_chat.py
è®¿é—®: http://localhost:5000
"""

import torch, json, threading
from flask import Flask, request, jsonify, Response
from transformers import AutoTokenizer
from gpt import GPT
from lora import apply_lora, load_lora_weights
import os

# ============================================================
#  é…ç½®
# ============================================================
BASE_MODEL = "Qwen/Qwen2.5-1.5B"
MERGED_DIR = "out/sft/merged_model"
USE_LORA = True
LORA_PATH = "out/sft/lora_best.pt"
LORA_R, LORA_ALPHA = 16, 32
LORA_TARGETS = {"c_q", "c_v"}

MAX_NEW_TOKENS = 256   # ç¼©çŸ­ï¼Œé¿å…åºŸè¯è¿‡å¤š
TEMPERATURE = 0.7
TOP_K = 50
TOP_P = 0.9
REPETITION_PENALTY = 1.5  # åŠ å¤§æƒ©ç½š

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.bfloat16 if (DEVICE == "cuda" and torch.cuda.is_bf16_supported()) else torch.float16

lock = threading.Lock()

# ============================================================
#  æ¨¡å‹åŠ è½½
# ============================================================
def load_model():
    merged_ckpt = os.path.join(MERGED_DIR, "best_model.pt")

    if os.path.exists(merged_ckpt):
        print(f"åŠ è½½åˆå¹¶æ¨¡å‹: {merged_ckpt}")
        tok = AutoTokenizer.from_pretrained(MERGED_DIR, trust_remote_code=True)
        ckpt = torch.load(merged_ckpt, map_location="cpu", weights_only=False)
        mdl = GPT(ckpt["config"])
        mdl.load_state_dict(ckpt["state_dict"])
    elif USE_LORA:
        print(f"åŠ è½½ base + LoRA")
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
        mdl = GPT.from_pretrained(BASE_MODEL)
        apply_lora(mdl, r=LORA_R, alpha=LORA_ALPHA, targets=LORA_TARGETS)
        load_lora_weights(mdl, LORA_PATH)
    else:
        print(f"åŠ è½½ HF åŸå§‹æƒé‡")
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
        mdl = GPT.from_pretrained(BASE_MODEL)

    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    mdl = mdl.to(DTYPE).to(DEVICE).eval()
    total = sum(p.numel() for p in mdl.parameters()) / 1e6
    print(f"æ¨¡å‹å·²åŠ è½½: {total:.1f}M å‚æ•°, è®¾å¤‡: {DEVICE}")
    return mdl, tok

model, tokenizer = load_model()

# ============================================================
#  æ¨ç†
# ============================================================
def apply_repetition_penalty(logits, generated_ids, penalty):
    """å¯¹å·²ç”Ÿæˆè¿‡çš„ token æ–½åŠ é‡å¤æƒ©ç½š, logits shape: [vocab_size]"""
    if penalty == 1.0 or len(generated_ids) == 0:
        return logits
    unique_ids = list(set(generated_ids))
    score = logits[unique_ids]
    score = torch.where(score > 0, score / penalty, score * penalty)
    logits[unique_ids] = score
    return logits


def detect_repetition(ids, min_pattern=8, max_check=100):
    """æ£€æµ‹æœ€è¿‘ç”Ÿæˆçš„ token æ˜¯å¦é™·å…¥å¾ªç¯"""
    if len(ids) < min_pattern * 2:
        return False
    recent = ids[-max_check:]
    for plen in range(min_pattern, len(recent) // 2 + 1):
        pattern = recent[-plen:]
        prev = recent[-2 * plen:-plen]
        if pattern == prev:
            return True
    return False


def trim_verbose(text):
    """æˆªæ–­é‡å¤/åºŸè¯å°¾å·´ï¼šåœ¨æœ€åä¸€ä¸ªå®Œæ•´å¥ç»“æŸå¤„æˆªæ–­"""
    # å¦‚æœæ–‡æœ¬è¾ƒçŸ­ä¸å¤„ç†
    if len(text) < 80:
        return text

    # æ‰¾é‡å¤æ®µè½ï¼šå¦‚æœæŸä¸ªå¥å­ç‰‡æ®µé‡å¤å‡ºç°ï¼Œåœ¨ç¬¬äºŒæ¬¡å‡ºç°å‰æˆªæ–­
    sentences = []
    for sep in ['ã€‚', 'ï¼', 'ï¼Ÿ', '\n']:
        text = text.replace(sep, sep + '\x00')
    parts = [s.strip() for s in text.split('\x00') if s.strip()]

    seen = set()
    result = []
    for part in parts:
        # å–æ ¸å¿ƒå†…å®¹ï¼ˆå»æ‰æ ‡ç‚¹ï¼‰åšå»é‡key
        key = part.replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼', '').replace('ï¼Ÿ', '').strip()
        if len(key) > 10 and key in seen:
            break  # é‡åˆ°é‡å¤å¥å­å°±åœ
        if len(key) > 10:
            seen.add(key)
        result.append(part)

    trimmed = ''.join(result)

    # é¢å¤–ï¼šç æ‰å¸¸è§åºŸè¯ç»“å°¾æ¨¡å¼
    cut_patterns = [
        "å¦‚æœæ‚¨è¿˜æœ‰", "å¦‚æœä½ è¿˜æœ‰", "å¦‚æœæœ‰ä»»ä½•", "å¸Œæœ›æˆ‘èƒ½",
        "è¯·éšæ—¶å‘Šè¯‰", "æ„Ÿè°¢æ‚¨çš„", "ç¥æ‚¨å¥½è¿", "æœŸå¾…ä¸‹æ¬¡",
        "å¦‚æœæ‚¨æƒ³", "å¦‚æœä½ æƒ³è®¨è®º", "æ¬¢è¿ç»§ç»­", "è¯·ç»§ç»­æé—®",
    ]
    for pat in cut_patterns:
        idx = trimmed.find(pat)
        if idx > 20:  # ç¡®ä¿ä¸æ˜¯æ•´æ®µéƒ½è¢«ç 
            trimmed = trimmed[:idx].rstrip('ï¼Œ,ã€ ')
            break

    return trimmed.strip()


def generate_reply(messages):
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    ids = torch.tensor([tokenizer.encode(text)], dtype=torch.long, device=DEVICE)
    inp_len = ids.shape[1]

    stop_ids = [tokenizer.eos_token_id]
    for t in ["<|im_end|>", "<|endoftext|>"]:
        tid = tokenizer.convert_tokens_to_ids(t)
        if tid != tokenizer.unk_token_id:
            stop_ids.append(tid)

    # æ‰‹åŠ¨é€ token ç”Ÿæˆï¼Œä»¥ä¾¿åŠ  repetition penalty + é‡å¤æ£€æµ‹
    generated = []
    with lock:
        for _ in range(MAX_NEW_TOKENS):
            idx_cond = ids if ids.size(1) <= model.config.max_seq_len else ids[:, -model.config.max_seq_len:]
            logits, _ = model(idx_cond)
            logits = logits[:, -1, :]  # [1, vocab]

            # é‡å¤æƒ©ç½š (åœ¨ [vocab] ç»´åº¦ä¸Šæ“ä½œ)
            logits_1d = logits[0]  # [vocab]
            logits_1d = apply_repetition_penalty(logits_1d, generated, REPETITION_PENALTY)
            logits = logits_1d.unsqueeze(0)  # [1, vocab]

            # temperature
            if TEMPERATURE > 0:
                logits = logits / TEMPERATURE

            # top-k
            if TOP_K > 0:
                v, _ = torch.topk(logits, min(TOP_K, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float("Inf")

            # top-p
            if TOP_P < 1.0:
                sorted_logits, sorted_idx = torch.sort(logits, descending=True)
                cum_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                remove = cum_probs > TOP_P
                remove[:, 1:] = remove[:, :-1].clone()
                remove[:, 0] = False
                indices_to_remove = remove.scatter(1, sorted_idx, remove)
                logits[indices_to_remove] = -float("Inf")

            probs = torch.softmax(logits, dim=-1)
            next_id = torch.multinomial(probs, num_samples=1)  # [1, 1]
            ids = torch.cat([ids, next_id], dim=1)

            token_id = next_id.item()
            generated.append(token_id)

            # EOS æ£€æŸ¥
            if token_id in stop_ids:
                break

            # é‡å¤å¾ªç¯æ£€æµ‹: æå‰ç»ˆæ­¢
            if len(generated) > 30 and detect_repetition(generated):
                break

    # æˆªæ–­ stop token
    for sid in stop_ids:
        if sid in generated:
            generated = generated[:generated.index(sid)]

    resp = tokenizer.decode(generated, skip_special_tokens=True)
    for tag in ["<|im_end|>", "<|endoftext|>", "<|im_start|>"]:
        if tag in resp:
            resp = resp[:resp.index(tag)]
    resp = trim_verbose(resp)
    return resp.strip()

# ============================================================
#  Flask
# ============================================================
app = Flask(__name__)

HTML = r"""<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>GPT Chat</title>
<style>
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body { font-family: -apple-system, "Segoe UI", sans-serif; background: #0f0f1a; color: #e0e0e0; height: 100vh; display: flex; flex-direction: column; }
  .header { padding: 16px 24px; background: linear-gradient(135deg, #1a1a2e, #16213e); border-bottom: 1px solid #2a2a4a; text-align: center; }
  .header h1 { font-size: 20px; background: linear-gradient(90deg, #667eea, #764ba2); -webkit-background-clip: text; -webkit-text-fill-color: transparent; }
  .header p { font-size: 12px; color: #888; margin-top: 4px; }
  #chat { flex: 1; overflow-y: auto; padding: 20px; display: flex; flex-direction: column; gap: 16px; }
  .msg { max-width: 80%; padding: 12px 16px; border-radius: 16px; line-height: 1.6; font-size: 14px; white-space: pre-wrap; word-break: break-word; animation: fadeIn .3s; }
  .user { align-self: flex-end; background: linear-gradient(135deg, #667eea, #764ba2); color: #fff; border-bottom-right-radius: 4px; }
  .bot { align-self: flex-start; background: #1e1e36; border: 1px solid #2a2a4a; border-bottom-left-radius: 4px; }
  .bot pre { background: #12121f; padding: 10px; border-radius: 8px; overflow-x: auto; margin: 8px 0; font-size: 13px; }
  .bot code { font-family: "Fira Code", monospace; font-size: 13px; }
  .typing { align-self: flex-start; padding: 12px 20px; background: #1e1e36; border: 1px solid #2a2a4a; border-radius: 16px; }
  .typing span { display: inline-block; width: 8px; height: 8px; background: #667eea; border-radius: 50%; margin: 0 2px; animation: bounce .6s infinite alternate; }
  .typing span:nth-child(2) { animation-delay: .2s; }
  .typing span:nth-child(3) { animation-delay: .4s; }
  @keyframes bounce { to { transform: translateY(-8px); opacity: .4; } }
  @keyframes fadeIn { from { opacity: 0; transform: translateY(8px); } to { opacity: 1; transform: translateY(0); } }
  .input-area { padding: 16px 20px; background: #1a1a2e; border-top: 1px solid #2a2a4a; display: flex; gap: 10px; }
  #input { flex: 1; padding: 12px 16px; border-radius: 24px; border: 1px solid #2a2a4a; background: #12121f; color: #e0e0e0; font-size: 14px; outline: none; resize: none; max-height: 120px; }
  #input:focus { border-color: #667eea; }
  #send { padding: 0 24px; border-radius: 24px; border: none; background: linear-gradient(135deg, #667eea, #764ba2); color: #fff; font-size: 14px; cursor: pointer; transition: opacity .2s; }
  #send:hover { opacity: .85; }
  #send:disabled { opacity: .4; cursor: not-allowed; }
  #clear { padding: 0 16px; border-radius: 24px; border: 1px solid #2a2a4a; background: transparent; color: #888; font-size: 13px; cursor: pointer; }
  #clear:hover { border-color: #667eea; color: #e0e0e0; }
</style>
</head>
<body>
<div class="header">
  <h1>ğŸ¤– GPT Chat</h1>
  <p>æƒé‡æ¥è‡ªQwen2.5-1.5B + è‡ªå®šä¹‰æ¨¡å‹ + LoRA SFT</p>
</div>
<div id="chat"></div>
<div class="input-area">
  <textarea id="input" rows="1" placeholder="è¾“å…¥æ¶ˆæ¯..." onkeydown="if(event.key==='Enter'&&!event.shiftKey){event.preventDefault();send()}"></textarea>
  <button id="send" onclick="send()">å‘é€</button>
  <button id="clear" onclick="clearChat()">æ¸…ç©º</button>
</div>
<script>
const chat = document.getElementById('chat');
const input = document.getElementById('input');
const sendBtn = document.getElementById('send');
let history = [];

// è‡ªåŠ¨è°ƒæ•´è¾“å…¥æ¡†é«˜åº¦
input.addEventListener('input', function() {
  this.style.height = 'auto';
  this.style.height = Math.min(this.scrollHeight, 120) + 'px';
});

function addMsg(role, text) {
  const d = document.createElement('div');
  d.className = 'msg ' + role;
  // ç®€å•çš„ code block æ¸²æŸ“
  text = text.replace(/```(\w*)\n([\s\S]*?)```/g, '<pre><code>$2</code></pre>');
  text = text.replace(/`([^`]+)`/g, '<code>$1</code>');
  d.innerHTML = text;
  chat.appendChild(d);
  chat.scrollTop = chat.scrollHeight;
  return d;
}

function showTyping() {
  const d = document.createElement('div');
  d.className = 'typing';
  d.id = 'typing';
  d.innerHTML = '<span></span><span></span><span></span>';
  chat.appendChild(d);
  chat.scrollTop = chat.scrollHeight;
}

function hideTyping() {
  const el = document.getElementById('typing');
  if (el) el.remove();
}

async function send() {
  const text = input.value.trim();
  if (!text) return;
  input.value = '';
  input.style.height = 'auto';
  sendBtn.disabled = true;

  addMsg('user', text);
  showTyping();

  try {
    const res = await fetch('/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ message: text, history: history })
    });
    const data = await res.json();
    hideTyping();

    if (data.error) {
      addMsg('bot', 'âš ï¸ ' + data.error);
    } else {
      addMsg('bot', data.reply);
      history.push([text, data.reply]);
      if (history.length > 5) history = history.slice(-5);
    }
  } catch(e) {
    hideTyping();
    addMsg('bot', 'âš ï¸ è¯·æ±‚å¤±è´¥: ' + e.message);
  }
  sendBtn.disabled = false;
  input.focus();
}

function clearChat() {
  history = [];
  chat.innerHTML = '';
}
</script>
</body>
</html>"""

@app.route("/")
def index():
    return Response(HTML, content_type="text/html")

@app.route("/chat", methods=["POST"])
def chat_api():
    try:
        data = request.json
        msg = data.get("message", "").strip()
        hist = data.get("history", [])
        if not msg:
            return jsonify({"error": "æ¶ˆæ¯ä¸èƒ½ä¸ºç©º"})

        messages = []
        for u, b in hist:
            messages.append({"role": "user", "content": u})
            messages.append({"role": "assistant", "content": b})
        messages.append({"role": "user", "content": msg})

        reply = generate_reply(messages)
        return jsonify({"reply": reply})
    except Exception as e:
        return jsonify({"error": str(e)})

if __name__ == "__main__":
    print("\n" + "=" * 50)
    print("  ç½‘é¡µèŠå¤©å·²å¯åŠ¨: http://localhost:5000")
    print("=" * 50 + "\n")
    app.run(host="0.0.0.0", port=5000, debug=False)